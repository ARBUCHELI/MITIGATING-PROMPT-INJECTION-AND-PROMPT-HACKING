1. Prompt Hacking and Mitigation
--------------------------------

* Prompt Hacking Techniques
---------------------------

Let's Pretend
-------------
A hacker can ask ChatGPT to assist with doing something ilegall by telling the system that he is writing a fiction book.

Prompt Leaking
--------------
Leak internal data from a company's resources in order to gain access to sensitive or confidential information.

Document Completion
-------------------
A hacker can provide the system, instructions asking the LLMS to complete some document about internal data you want to know.

Jail Breaking
-------------
Techniques hacker use to bypass existing migitations and get the LLMS to disregard existing guardrails built into their system.

A target without proper mitigation measures becomes irresistible to hackers looking to prove themselves.

------------------------------------------------------------------------------------------------------------------------------------------------------------------

* Mitigating prompt hacking attacks
-----------------------------------
This should be part of a proper cybersecurity strategy.

	- Filtering the input (words or phrases like hypotetically, behave as a, let's pretend, ignore previous requests)
	- Translate binary into English (with hidden attacks inside).
	- Splitting the input, and then asking the system to concanate it or reverse it and manipulating in diverse forms.

Other malicious techniques:

	- Receiving malicious code from an url, or a an external third party.

Try to only allow short prompts.

Isolate the user input before you feed it to the LLM.

Separate the user input from your own instructions --> This can give the instructions less weight than your own instructions.

Try to scape characters in the prompts in order to replace opening or closing tags.

Proxy the prompts to another LLM system to detect anomalies could be a good strategy too.



